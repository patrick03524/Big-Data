{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark NLP Basic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaVmDt1TEXdh"
      },
      "source": [
        "# Spark NLP\n",
        "### Configuración inicial de Spark NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtCa0sZ8EXdj"
      },
      "source": [
        "# Integrantes:\n",
        "\n",
        "* Patrick Xavier Marquez Choque\n",
        "* Jean Carlo Cornejo Cornejo\n",
        "\n",
        "Primero es necesario la instalación de la propia librería luego de esto podremos utilizar los modelos pre-entrenados y el pipeline de Spark NLP\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG_nuJGw68A6"
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyMMD_upEfIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9fba96e-8c0f-4045-f9a6-0dcef35cb987"
      },
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-01 19:57:25--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://setup.johnsnowlabs.com/colab.sh [following]\n",
            "--2021-12-01 19:57:25--  https://setup.johnsnowlabs.com/colab.sh\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2021-12-01 19:57:26--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1275 (1.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-01 19:57:26 (22.7 MB/s) - written to stdout [1275/1275]\n",
            "\n",
            "setup Colab for PySpark 3.0.3 and Spark NLP 3.3.4\n",
            "Installing PySpark 3.0.3 and Spark NLP 3.3.4\n",
            "\u001b[K     |████████████████████████████████| 209.1 MB 62 kB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 45.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 56.2 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5in-TmzGEXdk",
        "outputId": "afeb70d3-3147-4b86-f778-8d53c6d51c87"
      },
      "source": [
        "import sparknlp\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: {}\".format(sparknlp.version()))\n",
        "print(\"Apache Spark version: {}\".format(spark.version))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version: 3.3.4\n",
            "Apache Spark version: 3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt1KiTMFEXdp"
      },
      "source": [
        "from sparknlp.pretrained import PretrainedPipeline "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtGm-OY4EXds"
      },
      "source": [
        "Let's use Spark NLP pre-trained pipeline for `named entity recognition`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNu3meQKEXdu",
        "outputId": "b68a6c0c-67a4-4676-9043-9a656cb65da2"
      },
      "source": [
        "pipeline = PretrainedPipeline('recognize_entities_dl', 'en')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recognize_entities_dl download started this may take some time.\n",
            "Approx size to download 160.1 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMzyLyftEXdy"
      },
      "source": [
        "result = pipeline.annotate('President Biden represented Delaware for 36 years in the U.S. Senate before becoming the 47th Vice President of the United States.') "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ark1N0GEXd1",
        "outputId": "e25c7c31-c0c4-43cd-d10a-f96396c7eb1c"
      },
      "source": [
        "print(result['ner'])\n",
        "print(result['entities'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'B-PER', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n",
            "['Biden', 'Delaware', 'U.S', 'Senate', 'United States']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5ivlUOaXQVl"
      },
      "source": [
        "Let's try another Spark NLP pre-trained pipeline for `named entity recognition`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxWfmz_sXWWv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44917b9d-09ec-4ba1-f97a-fc34faa5a997"
      },
      "source": [
        "pipeline = PretrainedPipeline('onto_recognize_entities_bert_tiny', 'en')\n",
        "\n",
        "result = pipeline.annotate(\"Johnson first entered politics when elected in 2001 as a member of Parliament. He then served eight years as the mayor of London, from 2008 to 2016, before rejoining Parliament.\")\n",
        "\n",
        "print(result['ner'])\n",
        "print(result['entities'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "onto_recognize_entities_bert_tiny download started this may take some time.\n",
            "Approx size to download 30.2 MB\n",
            "[OK!]\n",
            "['B-PERSON', 'B-ORDINAL', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'B-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'B-GPE', 'O', 'B-DATE', 'O', 'B-DATE', 'O', 'O', 'O', 'B-ORG']\n",
            "['Johnson', 'first', '2001', 'Parliament.', 'eight years', 'London,', '2008', '2016', 'Parliament.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EKcEN_oEXd9"
      },
      "source": [
        "Let's use Spark NLP pre-trained pipeline for `sentiment` analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4ZXQDnlEXd-",
        "outputId": "6e740c85-cf5c-45fe-aa52-b1a157e668b7"
      },
      "source": [
        "pipeline = PretrainedPipeline('analyze_sentimentdl_glove_imdb', 'en')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "analyze_sentimentdl_glove_imdb download started this may take some time.\n",
            "Approx size to download 155.3 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73O-w8IYEXeC"
      },
      "source": [
        "result = pipeline.annotate(\"Harry Potter is a great movie.\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joIUX2P4EXeJ",
        "outputId": "cec81508-3b7f-4799-b0c3-7843d019e067"
      },
      "source": [
        "print(result['sentiment'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pos']\n"
          ]
        }
      ]
    }
  ]
}